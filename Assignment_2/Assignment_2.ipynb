{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Modelling Assignment 2  <font color='red'>Deadline = 30 april 23:59</font>\n",
    "\n",
    "In totaal kan je voor deze opdracht __81 punten__ halen + 5 bonuspunten.\n",
    "\n",
    "## Fitting functions\n",
    "\n",
    "Hieronder volgen wat korte vragen en uitleg over model fitting ter voorbereiding op de verdere assignments. Een deel van de thoerie komt misschien bekend voor. Het is de bedoeling dat je op basis van deze simpele opdracht in ziet hoe het fitten van een cognitief model, niet veel verschilt van het fitten van wiskundige formule fitten op data. In de basis is het idee dat je door een bepaalde methode de parameters van de desbetreffende functie kan vinden die de data, of het gedrag van mensen, zo goed mogelijk kan beschrijven. \n",
    "\n",
    "Wegens historische redenen is het eerste deel van deze opdracht nog in het Engels, je mag in het Nederlands of in het Engels antwoorden. \n",
    "\n",
    "Bij elke vraag staat de hoeveelheid punten die je er voor kan krijgen. Geef antwoorden in blokken met code of met tekst. Gebruik voor antwoorden met tekst de \">\" voor blockquotes en geef bij elke vraag ook __kort uitleg__ als hier om wordt gevraagd. __Let op__: soms staan er meerdere vragen bij een onderdeel, lees de tekst dus nauwkeurig. \n",
    "\n",
    "Sla het uiteindelijke notebook op met jullie studentnummers en achternamen in de filenaam: `studentnummer_achternaam_opdrachtnummer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Fitting functions\n",
    "\n",
    "Voor deze inleidende opdracht schrijven we verschillende functies en gaan deze op data \"fitten\". De meeste concepten hier moeten bekend zijn uit de cursus *Leren*, behalve dat we de *SciPy*-library zullen gebruiken om in de functies te fitten op data. Met fitten bedoelen we de optimale waarden vinden van de parameters in de functie zodat deze de data het best mogelijk benadert. De *SciPy*-library komt goed van pas bij latere opdrachten, dus het is handig om hier alvast wat elementen te introduceren.\n",
    "\n",
    "### Generating simulation data\n",
    "\n",
    "Assume there is some model that is defined by the following function:\n",
    "\n",
    "$$y = e^{\\frac{3 x}{20}}$$\n",
    "\n",
    "Imagine that this represents the relationship between the temperature (`x`) and the amount of ice cream sold (`y`) in a small ice cream store. The higher the temperature the more ice-cream is sold. For an ice-creamshop owner it would be good to know this exact relationship, such that she can predict how much to have in store when the temperature is changing. \n",
    "\n",
    "Of course, such a relationship cannot be found in a book but must be estimated from the data. So in the first step we will generate some data.  \n",
    "\n",
    "* Write a function `curve` which implements this model\n",
    "* Generate a uniformly spaced set of 50 samples over the interval `[1, 13]` using [linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)\n",
    "* Apply the curve function to the entire *ndarray* of samples using [vectorize](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html)\n",
    "\n",
    "The shop owner is taking note of the ice cream sold on every day but of course such measurements are noisy, not on each day that it is 18 degrees celcius will the exact same amount of people show up. Their ice cream buying behavior will be determined not only by the temperature but also many other factors (e.g. how much money the customers have in their account) that are unknown to the shop owner. Therefore we will add some noise to the data:\n",
    "\n",
    "* Create an *ndarray* of noise from a Gaussian distribution with $\\mu = 0.0$ and $\\sigma = 1.0$ using [random.normal](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html)\n",
    "* Add the noise to the curve results to create the artificial simulation data\n",
    "* Plot the actual underlying curve as a line and the simulated data as dots, both in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(seed=99999)\n",
    "\n",
    "def curve(x):\n",
    "    return math.e**(3*x/20)\n",
    "\n",
    "samples = np.linspace(1, 13)\n",
    "model_data = np.vectorize(curve)(samples)\n",
    "\n",
    "noise = np.random.normal(size=samples.shape)\n",
    "sim_data = model_data + noise\n",
    "\n",
    "plt.plot(samples, model_data, label='Actual curve')\n",
    "plt.plot(samples, sim_data, 'o', label='Measured data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q.0a Fitting a linear model (2 points)\n",
    "\n",
    "Now we'll use *SciPy* to fit a line through this simulation data. In *Leren* we needed to compute the partial derivative of the error function with respect to each of the model parameters to know how to modify those parameters to get the derivates (close to) zero. Here we will just define what the error function is, and let *SciPy* do the actual minimization work. Let's assume that we are looking at the relationship between the temperature and ice-cream sales volume. We expect that the higher the temperature the more icecream will be sold. \n",
    "\n",
    "The first model will be a very basic linear one, with only 1 parameter `a`. This model assumes there is a linear relationship between variables x and y:\n",
    "\n",
    "$$y = ax$$\n",
    "\n",
    "*(note: this line will always pass through the origin, in other words the intercept is zero)*\n",
    "\n",
    "* Write a function `linear` which implements this model\n",
    "\n",
    "It is our goal to find the optimal value for $\\alpha$.\n",
    "\n",
    "* Write a function `MSE` which computes the *Mean Squared Error* of an array of model estimates `y_hat` and an array of observed data `y`\n",
    "\n",
    "Already provided is a general function called `MSE_fit_func` which takes 4 arguments:\n",
    "1. *params:* A set of model parameters for the model function\n",
    "2. *func:* The model function that is being applied\n",
    "3. *x:* An array of x values\n",
    "4. *y:* An array of y values\n",
    "\n",
    "This function applies the model function to the *x* values using the model parameters and computes the resulting *MSE* with the observed *y* values. Assuming the data (i.e. the *x* and *y* values) and the model function are fixed, the model parameter(s) can be varied and the function will return an *MSE* for each parameter setting.\n",
    "\n",
    "The *SciPy* module *optimize* has a lot of functions to solve exactly this type of problem, where you change some parameters in order to minimize some function. As the current linear model has only one variable, we'll use the function [minimize_scalar](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar)\n",
    "\n",
    "The function we want to minimize is the `MSE_fit_func` (i.e. the MSE of applying the model function to the data), and as the minimization method we'll use `Brent`. There are quite a few different minimization methods *SciPy* offers, but we won't worry about their differences for now and just use the simple `Brent` method. All minimization functions will try and minimize the returned value of the function by varying its **first argument**. The other function arguments will remain fixed and may be provided with optional argument `args`.\n",
    "\n",
    "* Finish the call to `minimize_scalar` by filling in the correct values for `args` to fit the linear model to the simulation data generated in *Q1*.\n",
    "\n",
    "The variable `fit` will now be an [OptimizeResult](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult) object.\n",
    "\n",
    "* Print the variable `fit` and some of its attributes to inspect the results of the minimization\n",
    "* Plot the simulation data as dots and the fitted linear model as a line, both in the same plot\n",
    "* Print the value of `a` is that minimizes the linear model MSE on your data\n",
    "* Print the value of `fun` which is the MSE of the best fitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mMSE_fit_func\u001b[39m(params, func, x, y):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MSE(func(x, params), y)\n\u001b[1;32m---> 14\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMSE_fit_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBrent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:794\u001b[0m, in \u001b[0;36mminimize_scalar\u001b[1;34m(fun, bracket, bounds, args, method, tol, options)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(fun, args\u001b[38;5;241m=\u001b[39margs, bracket\u001b[38;5;241m=\u001b[39mbracket, bounds\u001b[38;5;241m=\u001b[39mbounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrent\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_scalar_brent(fun, bracket, args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbounded\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:2396\u001b[0m, in \u001b[0;36m_minimize_scalar_brent\u001b[1;34m(func, brack, args, xtol, maxiter, **unknown_options)\u001b[0m\n\u001b[0;32m   2393\u001b[0m brent \u001b[38;5;241m=\u001b[39m Brent(func\u001b[38;5;241m=\u001b[39mfunc, args\u001b[38;5;241m=\u001b[39margs, tol\u001b[38;5;241m=\u001b[39mtol,\n\u001b[0;32m   2394\u001b[0m               full_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, maxiter\u001b[38;5;241m=\u001b[39mmaxiter)\n\u001b[0;32m   2395\u001b[0m brent\u001b[38;5;241m.\u001b[39mset_bracket(brack)\n\u001b[1;32m-> 2396\u001b[0m \u001b[43mbrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2397\u001b[0m x, fval, nit, nfev \u001b[38;5;241m=\u001b[39m brent\u001b[38;5;241m.\u001b[39mget_result(full_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2399\u001b[0m success \u001b[38;5;241m=\u001b[39m nit \u001b[38;5;241m<\u001b[39m maxiter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misnan(x) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(fval))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:2180\u001b[0m, in \u001b[0;36mBrent.optimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2178\u001b[0m     \u001b[38;5;66;03m# set up for optimization\u001b[39;00m\n\u001b[0;32m   2179\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m-> 2180\u001b[0m     xa, xb, xc, fa, fb, fc, funcalls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_bracket_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2181\u001b[0m     _mintol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mintol\n\u001b[0;32m   2182\u001b[0m     _cg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cg\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:2154\u001b[0m, in \u001b[0;36mBrent.get_bracket_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;66;03m### BEGIN core bracket_info code ###\u001b[39;00m\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;66;03m### carefully DOCUMENT any CHANGES in core ##\u001b[39;00m\n\u001b[0;32m   2153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m brack \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2154\u001b[0m     xa, xb, xc, fa, fb, fc, funcalls \u001b[38;5;241m=\u001b[39m \u001b[43mbracket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(brack) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   2156\u001b[0m     xa, xb, xc, fa, fb, fc, funcalls \u001b[38;5;241m=\u001b[39m bracket(func, xa\u001b[38;5;241m=\u001b[39mbrack[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   2157\u001b[0m                                                xb\u001b[38;5;241m=\u001b[39mbrack[\u001b[38;5;241m1\u001b[39m], args\u001b[38;5;241m=\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:2606\u001b[0m, in \u001b[0;36mbracket\u001b[1;34m(func, xa, xb, args, grow_limit, maxiter)\u001b[0m\n\u001b[0;32m   2604\u001b[0m fa \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39m(xa,) \u001b[38;5;241m+\u001b[39m args)\n\u001b[0;32m   2605\u001b[0m fb \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39m(xb,) \u001b[38;5;241m+\u001b[39m args)\n\u001b[1;32m-> 2606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mfa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfb\u001b[49m):                      \u001b[38;5;66;03m# Switch so fa > fb\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m     xa, xb \u001b[38;5;241m=\u001b[39m xb, xa\n\u001b[0;32m   2608\u001b[0m     fa, fb \u001b[38;5;241m=\u001b[39m fb, fa\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def linear(x, a):\n",
    "    # TO DO\n",
    "    return\n",
    "\n",
    "def MSE(y_hat, y):\n",
    "    # TO DO\n",
    "    return\n",
    "\n",
    "def MSE_fit_func(params, func, x, y):\n",
    "    return MSE(func(x, params), y)\n",
    "\n",
    "fit = optimize.minimize_scalar(MSE_fit_func, method='Brent', args=(linear, samples, sim_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Q0.b Fitting a polynomial model (2 points) \n",
    "\n",
    "Now, the linear model is a pretty good fit, but the shopkeeper is not satisfied and a math hobbyst and plans to create a new model using a $4^{th}$-order polynomial function:\n",
    "\n",
    "$$y=b_0+b_1x+b_2x^2+b_3x^3+b_4x^4$$\n",
    "\n",
    "* Write a function `polynomial` which implements this model, with the argument *b* being an *ndarray* containing all model parameters $b_0 \\dots b_4$\n",
    "\n",
    "When minimizing multiple parameters, we'll need to provide starting values for `b`, from where the minimization function will start the search. For now, you should use `np.array([-5, 9, -4, 1, .01])` as the starting point and we'll come back to selecting sensible starting values for the parameters later.\n",
    "\n",
    "* Use the function [minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize) to fit the polynomial model to the generated data from *Q1* using the `Nelder-Mead` method\n",
    "* Plot the simulation data as dots and the fitted polynomial model as a line, both in the same plot\n",
    "* Print the value of `b` is that minimizes the polynomial model MSE on your data\n",
    "* Print the value of `fun` which is the MSE of the best fitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial(x, b):\n",
    "    # TO DO\n",
    "    return\n",
    "\n",
    "fit2 = optimize.minimize(MSE_fit_func, np.array([-5, 9, -4, 1, .01]), method='Nelder-Mead',\n",
    "                        args=(polynomial, samples, sim_data))\n",
    "\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Q0.c Comparing models (4 points) \n",
    "\n",
    "Now, the linear model is a pretty good fit, but did the shopkeeper improve her model by making it more complex?\n",
    "\n",
    "* Compare the MSE of both models and report which one has the best fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shopkeeper did all is her work in the winter, preparing for the summer to come. Hence, only temperatures between 1 and 13. But now it is spring and new data is comming in. The shopkeeper will now use her best model to predict sales. However, she was surprised to learn what her model predicted!\n",
    "\n",
    "* report the prediction of the model for 18 degrees celcius (`x = 18`), how does that compare to the best day in winter (`x = 13`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shopkeeper is again collecting data for 50 days to get a better picture of what is going on. \n",
    "\n",
    "* Generate a uniformly spaced set of 50 samples over the interval `[13, 20]` using [linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html)\n",
    "* Apply the curve function to the entire *ndarray* of samples using [vectorize](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html)\n",
    "\n",
    "With the new data she compared the predictions of her original two models with what really happened. \n",
    "* Plot the simulation data as dots and the fitted linear and polynomial models in the new plot.\n",
    "* Print the value of `fun` for both models, which one is now (clearly) the better. Try to explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO print values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cognitive Model Fitting\n",
    "\n",
    "In cognitive science different models can represent different theories of how the mind works. These theories are thus formalized in mathematical equations. The different models will make different predictions about the cognitive processes of people and ultimately of their behavior. To differentiate between these models they are often fit to real behavioral (or brain) data, and their \"fit\" to the data is compared. It is common practice to furhter \"punish\" models in relation the number of free parameters that they have (to prevent **overfitting**, see debacle with the polynomial above). We will get back to model fitting at point 4 below. \n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# I. Q-Learning\n",
    "\n",
    "Q-learning algorithmes zijn gebaseerd op de Temporal Difference learning modellen die we eerder hebben besproken. Q-learning leert direct de associatie tussen states, actions en outcomes. De robot krijgt nu dus niet alleen maar beloningen, maar kan ook uitzoeken welke handeling de beste is geveven de situatie.\n",
    "\n",
    "Details over Q-learning zijn terug te vinden in de college slides en het hoofdstuk van Gureckis & Love [**computational reinforcement learning**](http://bradlove.org/papers/GureckisLovePress.pdf) en voor meer verdieping in het online boek van [**Sutton & Barto**](http://incompleteideas.net/book/bookdraft2018jan1.pdf) en dan met name hoofdstuk 6.\n",
    "\n",
    "Het leren in deze opdracht speelt zich af in een simpel **Markov Decision Process** met de volgende structuur:\n",
    "\n",
    "![](images/bandit_arms.png)\n",
    "\n",
    "In deze taak moet de robot telkens uit één van de schatkisten iets pakken. In sommige schatkisten zit meer geld dan in anderen, maar de robot weet in het begin nog niks over de schatkisten, en verwacht er maar weinig van. In elke ronde wordt uitkomst van een schatkist bepaald door een trekking van een waarde uit een normaalverdeling met een ander gemiddelde. Het is aan de robot om er achter te komen welke van de vier schatkisten het meeste oplevert. De uitkomsten van schatkisten verschillen in hun gemiddelde maar niet in de variantie (standaard deviatie). \n",
    "\n",
    "We gaan Q-learing gebruiken om te beschrijven hoe de robot leert om de beste keuze te maken:\n",
    "\n",
    "$$ Q(s_{t+1},a_{t+1})= Q(s_t,a_t) + αδ $$\n",
    "\n",
    "waarbij $s_t$ de state op tijdstip $t$ is, en $a_t$ de actie op tijdstip $t$. De acties is hier dus het kiezen van een van de kisten (totaal 4 mogelijke acties). Net als bij Rescorla-Wagner en Temporal Difference Learning is $\\alpha$ de learning rate, en $\\delta$ is hier de prediction error. \n",
    "\n",
    "**Let op:** In dit simpele experiment is er maar één state, waarin de robot telkens terugkeert na het maken van een keuze. Dit heeft als gevolg dat bij het leren geen rekening gehouden hoeft te worden met de actie in de volgende state gemaakt wordt. De standaard prediction-error:\n",
    "\n",
    "$$\\delta = r_{t+1} + \\gamma\\ max_a\\ Q(s_{t+1} , a) − Q(s_t , a_t)$$\n",
    "\n",
    "verandert dus simpelweg in:\n",
    "$$\\delta = r_{t+1} − Q(s_t , a_t)$$\n",
    "\n",
    "In het begin van het experiment heeft de robot geen enkele kennis van de wereld en geen enkele verwachtingen voor van het krijgen van beloningen. Voor elke schatkist geldt:\n",
    "\n",
    "$$Q(1)=Q(2)=Q(3)=Q(4)=0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.a (2 punten)\n",
    "\n",
    "Schrijf de Q-learning functie op die de nieuwe waarde Q uitrekent nadat de robot een schatkist heeft uitgekozen. Welke vrije variabele heeft deze functie en wat is de rol van deze variabele bij leren? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.b (2 punten)\n",
    "\n",
    "Stel, de robot selecteert schatkist 1 en krijgt een beloning van 2 munten. Wat is hierna de waarde van $Q(1)$? rapporteer dit voor\n",
    "$\\alpha=0.5$ en $\\alpha=0.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.c (8 punten)\n",
    "\n",
    "Schrijf een functie `q_learn` die als input, alpha ($\\alpha$), epsilon ($\\epsilon$) en rondes (trials) accepteert. De output van deze functie moet een lijst met $Q$ waarden zijn voor elke schatkist (1 t/m 4) in de wereld van de robot na het leren van een aantal rondes. We gaan er nu van uit dat de robot de $\\epsilon$-greedy keuze regel toepast (zie hieronder).\n",
    "\n",
    "* Initieer de verwachtingen van de robot voor de 4 keuzes:\n",
    "    * $Q(1) = Q(2) = Q(3) = Q(4) = 0$. \n",
    "* Initieer total_score = 0\n",
    "* Initieer keuze lijsten:\n",
    "    * `choice_1 = np.zeros(trials)`\n",
    "    * `choice_2 = np.zeros(trials)`\n",
    "    * `choice_3 = np.zeros(trials)`\n",
    "    * `choice_4 = np.zeros(trials)`\n",
    "* Initieer de beloningen voor de 4 schatkisten:\n",
    "    * Kist 1: mean=20, SD=4\n",
    "    * Kist 2: mean=30, SD=4\n",
    "    * Kist 3: mean=50, SD=4\n",
    "    * Kist 4: mean=70, SD=4\n",
    "* Creëer een for-loop over alle rondes:\n",
    "    * Elke ronde selecteert de robot een kist op basis van $\\epsilon$-greedy. Dus bij het exploreren wordt er een random kist gekozen, en bij het exploiteren wordt de kist met hoogste Q-value gekozen. Als meerdere Q-values de hoogste waarde hebben, dan wordt daar ook random uit gekozen. __Let op:__ e-greedy kan afwijken van de formules in het college. Dit is correct:\n",
    "```python\n",
    "        if random.random() < epsilon:\n",
    "        #   explore\n",
    "        else:\n",
    "        #   exploit\n",
    "```\n",
    "    \n",
    "    * Kijk wat de beloning is na maken van een keuze, en update de Q-value van die kist. \n",
    "    \n",
    "* Zorg dat deze functie in ieder geval de volgende lijsten als output heeft: final Q values, total_score, keuzelijst voor elke kist. Dus: <br>\n",
    "` return (Q, total_score, choice_1, choice_2, choice_3, choice_4)` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn(alpha, epsilon, trials=200):\n",
    "    # TO DO\n",
    "    return (Q, total_score, choice_1, choice_2, choice_3, choice_4, outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Laat met behulp van deze functie de robot 200 rondes leren over deze wereld (1 leer episode bestaat dus uit 200 rondes). Hoe zien de verwachtingen (Q-values) voor de schatkisten eruit aan het eind van het experiment? En wat is de totale score? Geef voor:\n",
    "\n",
    "1. $\\alpha = 0.1$ en $\\epsilon = 0.1$\n",
    "2. $\\alpha = 0.5$ en $\\epsilon = 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Laat nu voor beide modellen zien hoe de keuzes voor de verschillende kisten veranderen gedurende de trails door middel van 2 plots. <br>\n",
    "Voor het plotten van keuzes is het handig om naar het gemiddelde van bins van 10 trails tegelijk te kijken. Alvast wat code om je beetje op weg te helpen:\n",
    "```python\n",
    "\n",
    "res_05_01 = q_learn(0.5, 0.1)   # roep functie aan\n",
    "width = 10                      # de grootte van de bin\n",
    "\n",
    "# Hier knippen we de laatste (choicelist_1.size // width) elementen van de lijst\n",
    "# Dan reshapen we naar een matrix van X * width en nemen we de mean over de width axis\n",
    "# Hiermee krijgen we dus het gemiddelde aantal keer dat deze keuze gemaakt is voor width stappen\n",
    "result1 = res_05_01[2][:(res_05_01[2].size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "result2 = res_05_01[3][:(res_05_01[3].size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "result3 = res_05_01[4][:(res_05_01[4].size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "result4 = res_05_01[5][:(res_05_01[5].size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "\n",
    "# Plot\n",
    "plt.plot(result1, label=r\"$1$\")\n",
    "plt.plot(result2, label=r\"$2$\")\n",
    "plt.plot(result3, label=r\"$3$\")\n",
    "plt.plot(result4, label=r\"$4$\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"choice ratio\")\n",
    "plt.xlabel(\"trials * 10\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Welk van de twee modellen zit dichter bij de waarheid als je kijkt naar de Q-values? \n",
    "* Welk model verdient meer punten? \n",
    "* Leg aan de hand van de grafieken en Q-values uit waar de verschillen vandaan komen\n",
    "\n",
    "*(probeer verschillende random.seeds() om te kijken wat representatief is, kies er één voor je antwoord).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.d (4 punten)\n",
    "\n",
    "Laten we nu verder kijken naar de verschillende leerstrategieën (lees: combinatie van parameter waardes). Aangezien er veel random noise is, kan het zijn dat in één leerepisode de beste kist niet gevonden wordt, het is daarom goed om naar het gemiddelde te kijken van meerdere runs om een beter beeld te krijgen van een specifiek algoritme.  \n",
    "\n",
    "Schrijf nu een loop die `q_learn` 500 keer aanroept met een bepaalde parameter setting (en nog altijd 200 leerrondes) en sla telkens het totaal aantal punten op, zodat je aan het eind een lijst hebt van 500 totaal scores.\n",
    "\n",
    "Vergelijk het gemiddelde van de 500 totaalscores voor $\\alpha=0.1$, $\\alpha=0.3$ en $\\alpha=0.5$ met allemaal $\\epsilon=0.1$ voor alle experimenten. Gebruik geen random.seed(). Verklaar hoe de verschillen tot stand komen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# II. Exploration-Exploitation\n",
    "\n",
    "## 2.  $\\epsilon$-greedy\n",
    "\n",
    "We gaan nog wat dieper in op het exploration vs. exploitation dilemma. De robot gebruikt nu de meest simpele exploratie regel; $\\epsilon $-greedy. Laten we kijken hoe verschillende waardes voor  $\\epsilon$ uitwerking hebben op het aantal punten dat gewonnen wordt. \n",
    "\n",
    "Gebruik hier de functie van `q_learn` met $\\epsilon=0.05$, $\\epsilon=0.2$ en $\\epsilon=0.6$ en met $\\alpha=0.3$ voor alle experimenten. \n",
    "\n",
    "### Q2.a (4 punten)\n",
    "\n",
    "Kijk voor elke parameter setting weer naar de gemiddelde totaal score van 500 leer episodes. Waar ligt ongeveer het optimale niveau van exploratie? Probeer te verklaren waarom de alternatieven minder goede resultaten opleveren (b.v. waarom is een hoge of juist lage $\\epsilon$ niet goed)? Gebruik weer __geen__ random.seed()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.b (4 punten)\n",
    "Bij machine learning en Deep Neural Nets worden nog steeds vaak $\\epsilon$-greedy gebruikt, ook al is dit niet een exploratie regel die mensen lijken te gebruiken. Ook hier is het kiezen van een goede waarde voor $\\epsilon$ vaak het resultaat van trial en error. Het kan dus zo zijn dat de onderzoeker niet de optimale waarde kiest. Een regel die vaak wordt toegepast is dat de waarde van $\\epsilon$ afneemt gedurende het experiment. \n",
    "\n",
    "Pas nu `q_learn` zo aan dat de parameter $\\epsilon$ gedurende een leer episode steeds kleiner wordt. Dit kan bijvoorbeeld door elke ronde $\\epsilon$ met een vast percentage te verkleinen (denk aan iets tussen 0 en 10%), maar andere manieren zijn ook mogelijk. Sla dit model op als `q_learn_decay()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_decay(alpha, epsilon, trials=200, decay=0):\n",
    "    # TO DO\n",
    "    return (Q, total_score, choice_1, choice_2, choice_3, choice_4, outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.c (4 punten)\n",
    "\n",
    "Kijk nu naar een aantal beginwaarden voor parameter $\\epsilon$, en kijk welk model (`q_learn` of `q_learn_decay`) meer punten kan verdienen in de taak (verken waarden van $\\epsilon $ tussen .1 en .9). Gebruik weer het gemiddelde aantal punten over 500 leer episodes (en nog steeds 200 rondes per episode en $\\alpha =  0.3$). Welk model is het beste en waarom denk je dat dit zo is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Softmax\n",
    "\n",
    "Een andere zeer populaire methode om met het exploratie-explotatie dilemma om te gaan is de softmax beslis regel. Deze kan gebruikt worden om de waarschijnlijkheid uit te rekenen dat de robot een specifieke kist kiest. Bijvoorbeeld de waarschijnlijkheid dat de robot op een bepaald moment kist 1 kiest is:\n",
    "\n",
    "$$P(Q(1)) = \\frac{e(Q(1)*\\theta)}{\\sum_s e(Q(s)*\\theta)}$$\n",
    "\n",
    "Voor kist 2:\n",
    "\n",
    "$$P(Q(2)) = \\frac{e(Q(2)*\\theta)}{\\sum_s e(Q(s)*\\theta)}$$\n",
    "\n",
    "En natuurlijk $P(Q(1))+ P(Q(2)) +P(Q(3))+ P(Q(2)) = 100\\%$ want de robot kiest altijd één van de 4 opties, dus samen moeten dat 100% kans zijn.\n",
    "\n",
    "De waarde van $\\theta$ bepaalt de mate waarin het verschil in verwachte waarde (Q values) de kans op een bepaalde keuze vergroot. Deze parameter wordt ook wel de *inverse temperature* genoemd. Als de waarde van $\\theta$ *laag* is, is de temperatuur van de functie *hoog* en dan doet het verschil in Q values er niet zoveel toe, dus worden er meer random keuzes gemaakt. Oftewel, er zal meer gexploreerd worden. De term temperatuur komt van de analogie met ijzer smeden, bij een hoge temperatuur is het makkelijker te buigen. \n",
    "\n",
    "### Q3 (6 punten)\n",
    "\n",
    "Implementeer nu de softmax regel in de `q_learn` functie en geef deze de naam `q_learn_softmax`.\n",
    "\n",
    "* Gebruik elke ronde de $P(Q)$ informatie om de robot een kist te laten kiezen.\n",
    "* Zorg dat elke ronde de waarschijnlijkheid $P(Q)$ van het kiezen van elke kist wordt opgeslagen zodat we hier later weer naar kunnen kijken. \n",
    "\n",
    "Gebruik dit model weer om de gemiddelde score voor 500 episodes voor verschillende waardes van $\\theta$ (waardes tussen $0.01$ en $1$, op zn minst 5) met elkaar te vergelijken, met wederom 200 rondes per episode en een $\\alpha$ van $0.3$, gebruik makende van de methode die we hierboven ontwikkeld hebben. \n",
    "\n",
    "Wat is ongeveer de optimale waarde voor theta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_softmax(alpha, theta, trials=200):\n",
    "    # TO DO \n",
    "    return (Q, total_score, choice_1,choice_2,choice_3,choice_4, outcomes)\n",
    "\n",
    "# TO DO: vergelijk parameter waardes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We willen nu weten waar de verschillen vandaan komen. Laat voor $\\theta$ = $0.1$ en  $\\theta = 1$ zien wat de Q-values en totaal score is. Plot ook weer de keuzes per 10 bins (zelfde als bij vraag 1). Gebruik hier random.seed(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Wat laten deze waardes en grafieken je zien?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Upper Confidence Bound\n",
    "\n",
    "De softmax regel is vaak al beter dan $\\epsilon$-greedy, maar exploratie is nog steeds random. Als we kijken naar het gedrag van mensen, zien we dat mensen slim zijn en niet random verkennen (exploreren). Een zeer populaire methode om niet geheel random exploratie te implementeren is met de Upper Confidence Bound exploratie. \n",
    "\n",
    "Volgens deze regel krijgt een optie een bonus ($\\eta$) als het exploreert, met andere woorden: dit maak het model nieuwsgierig. UCB kan op verschillende manieren worden geimplementeerd, hier is een populaire versie UCB1:\n",
    "\n",
    "$$Q'(action) = Q(action)+ \\eta$$\n",
    "\n",
    "$$\\eta = \\sqrt{\\theta log(t)/N_t(action)}$$\n",
    "\n",
    "Waarbij $t$ staat voor ronde nummer in het experiment, and $N_t$ het aantal keer in totaal dat een bepaalde keuze (action) gemaakt is. De parameter $\\theta$ schaalt de grootte van de exploratie bonus (met een hogere $\\theta$ is de robot nieuwsgieriger). \n",
    "\n",
    "**Let op:** het algoritme kiest in het begin van het experiment elke optie op z'n minst een keer, daarna pas wordt de exploratie bonus toegepast. \n",
    "\n",
    "### Q4.a (6 punten)\n",
    "\n",
    "Implementeer nu de UCB exploratie regel op basis van de `q_learn` functie en geef deze de naam `q_learn_UCB`.\n",
    "\n",
    "* Gebruik elke ronde de $Q'(Action)$ informatie om de robot een kist te laten kiezen.\n",
    "* Zorg ook dat deze elke ronde  $\\eta$ van elke kist wordt opgeslagen zodat we hier later weer naar kunnen kijken. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def q_learn_UCB(alpha, theta, trials=200):\n",
    "    # TO DO                  \n",
    "    return (Q, total_score, choice_1,choice_2,choice_3,choice_4,outcomes,eta_1,eta_2,eta_3,eta_4,QQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "* Reken weer de gemiddelde score voor 500 episodes uit voor een $\\theta$ van 2 (default voor UCB1), met wederom 200 rondes per episode en een $\\alpha$ van $0.3$, gebruik makende van de methode die we hierboven ontwikkeld hebben.\n",
    "* Plot ook weer de keuzes per 10 bins (zoals bij vraag 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO gemiddelde\n",
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* Als je kijkt naar de implementatie, hoe verhoudt dit model zich tot de simpele versie van $\\epsilon$-greedy en soft-max? \n",
    "* Kijk ook naar het aantal punten dat het behaalt, waar komt het verschil in prestatie vandaan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.b (6 punten) \n",
    "De exploration-exploitation trade-off wordt al lange tijd onderzocht en is een van de meest interessante problemen in reinforcement learning. In dit voorbeeld was het best makkelijk voor het algoritme om de oplossing te vinden omdat de 4 bandits redelijk veel verschilden van elkaar, maar dit is niet altijd het geval. Kijk maar eens wat er gebeurt als je de standaard deviatie in `q_learn_UCB` van 4 naar 10 verandert, of als je de means dichter bij elkaar zet. Dan zal je ook zien dat je een hogere exploratie waarde $\\theta$ nodig hebt om tot de beste oplossing te komen.\n",
    "\n",
    "* Verander de bandits SD en means: <br>\n",
    "    `means = [20, 30, 35, 45]`<br>\n",
    "    `std_dev = 10`\n",
    "* Simuleer nu weer voor verschillende waardes van $\\theta$ het algoritme (waardes tussen $2$ en $50$, op zn minst 4);\n",
    "* Rapporteer de beste waarde.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_UCB_new(alpha, theta, trials=200):\n",
    "    # TO DO                     \n",
    "    return (Q, total_score, choice_1,choice_2,choice_3,choice_4,outcomes,eta_1,eta_2,eta_3,eta_4,QQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samenvatting\n",
    "\n",
    "In de simpele wereld van 4 bandits is nog met enige moeite uit te vinden welk niveau van exploratie optimaal is, maar dit kost veel tijd. Daarnaast is de echte wereld nog veel complexer, dus dan is het helemaal moeilijk om uit te zoeken wat het beste niveau van exploratie is. Het blijft dus altijd een vraag hoe slimme systemen bepalen hoeveel of weinig zij gaan exploreren. Bij mensen lijken hier ook grote individuele verschillen in te zitten: de een is avontuurlijker dan de ander en mensen passen hun exploratiegedrag aan op de omgeving.\n",
    "\n",
    "#### Curiosity based exploration in complex systems\n",
    "Voor een veel complexer maar interessant voorbeeld is het leuk om te kijken naar: [Curiosity-driven Exploration by Self-supervised Prediction](https://pathak22.github.io/noreward-rl/). In dit paper probeert een agent de omgeving zo te verkennen dat het op zoek gaat naar states waar het meest over te leren valt, net als UCB. Door alleen heel nieuwsgierig te zijn zorgt het algoritme er al voor dat het heel ver kan komen in bepaalde games, zonder dat beloningen (bv punten) daar een belangrijke rol in spelen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# III. Model fitting\n",
    "\n",
    "## 5. De vier schatkisten, menselijke keuzes\n",
    "\n",
    "We gaan nu kijken naar de resultaten van een echt experiment. We hebben de data van een proefpersoon die het bovenstaande experiment met de vier kisten echt heeft gespeeld. In de *L4_data_1.txt* file kunnen we terugvinden welke van de 4 opties zij gekozen had en hoeveel punten er vervolgens in elke trial verdient zijn. We gaan kijken met welke parameter waardes het gedrag van de proefpersonen het beste kan voorspellen als we gebruik maken van Q-learning met een softmax choice rule. De proefpersoon heeft 80 rondes gespeeld. \n",
    "\n",
    "Lees de data in uit *L4_data_1.txt* (staat in de Data map) met behulp van [loadtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html):\n",
    "```python\n",
    "with open(\"Data\\L4_data_1.txt\") as f:\n",
    "    data = np.loadtxt(f, dtype=int, delimiter=\"\\t\", skiprows=1)\n",
    "```\n",
    "\n",
    "We pasen de `q_learn_softmax` functie aan zodat je deze op de data van de proefpersonen kan fitten (zie eerdere opdracht over hoe functies gefit moeten worden en code onderaan voor meer hulp).\n",
    "\n",
    "We gaan ervanuit dat de proefpersonen enige ervaring hebben met dit type experiment en verwachten dat ze gemiddeld wel 40 punten per ronde gaan verdienen (alle Q’s starten op 40 ipv 0). Voor het fitten van het model maken we gebruik van `minimize` van scipy.optimize (`from scipy.optimize import minimize`) en we gaan proberen de *Log Likelihood* te optimaliseren (= maximaliseren). \n",
    "\n",
    "Wat we bij elke trial willen weten is de waarschijnlijkheid dat het model dezelfde keuze maakt als de proefpersoon. Hoe groter de kans (likelihood) dat het model correct kiest, hoe beter het model \"fit\". \n",
    "\n",
    "In het databestand van de proefpersoon kun je zien welke van de 4 kisten de proefpersoon koos. Dit kunnen we elke ronde vergelijken met de corresponderende $P(Q)$ volgens het model. In de eerste ronde zijn alle Q values nog gelijk dus zijn alle $P(Q) = 0.25$. Voor de eerste ronde geldt daarom automatisch dat de waarschijnlijkheid (likelihood) van de keuze van de proefpersoon ook 0.25 is, maar dat gaat veranderen naarmate er geleerd wordt.\n",
    "\n",
    "De output van deze functie moet de som van alle *log(P(Q(chosen)))* zijn. Let op, deze som wordt vermenigvuldigd met -1 omdat minimize de functie probeert te minimaliseren, en we zijn op zoek naar de max LL. \n",
    "\n",
    "`**example code**:`\n",
    "```python\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "with open(\"Data\\L4_data_1.txt\") as f:\n",
    "    data = np.loadtxt(f, dtype=int, delimiter=\"\\t\", skiprows=1)\n",
    "\n",
    "def q_learn_softmax_fit(params):\n",
    "    alpha, theta, init_value = params\n",
    "    nArms = 4\n",
    "    Q = np.array([init_value]*nArms)\n",
    "    LL = 0\n",
    "    \n",
    "    # loop over all trials\n",
    "    for row in data:\n",
    "        \n",
    "        # bereken de kans (probs) dat elke kist gekozen wordt\n",
    "        probs = np.power(np.e, theta*Q)\n",
    "        probs /= sum(probs)\n",
    "        \n",
    "        choice = row[1]-1   # de keuze van de proefpersoon\n",
    "        outcome = row[2]    # de uitkomst van de keuze\n",
    "        \n",
    "        # bereken de log likelihood van de keuze van de proefpersoon (choice)\n",
    "        # Gegeven de huidige Q values van het model, hoe groot was de kans (likelihood) \n",
    "        # dat deze specifieke kist gekozen werd?\n",
    "        LL += np.log(probs[choice]) # houd de som van alle LL bij. \n",
    "        \n",
    "        # Update de Q-values, gegeven de werkelijke keuze en ook de werkelijke outcome \n",
    "        # gebruik daarbij vrije parameter alpha, de learning rate, we proberen de optimale waarde hiervan te vinden\n",
    "        Q[choice] = Q[choice] + alpha * (outcome - Q[choice])\n",
    "    \n",
    "    # Schaal met -1 om de functie te kunnen minimizen\n",
    "    return -1*LL\n",
    "\n",
    "# minimize takes a few arguments (function, array of initial parameter values, minimization methods,\n",
    "# bounds are the bounds on each parameter; use bounds (same, same) to fix parameter to a single value\n",
    "res = minimize(q_learn_softmax_fit, \n",
    "               np.array([0.5, 0.5, 40]), \n",
    "               method='SLSQP',\n",
    "               bounds=[(0,1), (0,10), (40,40)], \n",
    "               options={'disp':True, 'ftol':1e-16})\n",
    "```\n",
    "\n",
    "### Q5.a (7 punten)\n",
    "\n",
    "* Gebruik deze functie en fit het model. \n",
    "* Welke parameterwaarden fitten de data van de proefpersoon het beste? \n",
    "* Wat kan je zeggen over hoe goed het model de keuzes van de proefpersoon voorspelt? (geef gemiddelde likelihood per trial en wat dat betekent) <br>\n",
    "\n",
    "    _Minimize geeft de uiteindelijke summed LL (negatief) van het best fittende model. Deze score kan je weer terugrekenen naar een gemiddelde likelihood  per trial: de kans dat het model juiste trial koos. Doe dit een beoordeel de uitkomst. Hint: deel de output door het aantal trials, de inverse van log(x) = $e^x$._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Q5.b (4 punten)\n",
    "\n",
    "Als we de functie een klein beetje herschrijven kunnen we deze gebruiken om in het hoofd van de proefpersoon te kijken. Nu willen we bijvoorbeeld weten welke Q-values zij aan de verschillende bandits toekent. \n",
    "\n",
    "Hiervoor is alleen een kleine verandering nodig, waarbij de functie nu niet meer de LogLikelihood als output heeft maar de lijst met Q values. Pas de functie `q_learn_softmax_fit` aan zodat deze de Q-values returnt en noem de nieuwe functie `q_learn_fitted_model`. Zorg hierbij dat de `params` lijst van argumenten ook hetzelfde blijft als bij de `q_learn_softmax_fit`.\n",
    "\n",
    "Nu hoef je ook niet meer de functie fitten of minimalizeren maar alleen aan te roepen, gebruik makende van de beste gevonden `params` van de vorige stap.\n",
    "\n",
    "`q_learn_fitted_model(res.x)`\n",
    "\n",
    "Rapporteer de Q values van deze proefpersoon. In werkelijkheid waren de gemiddelde waardes van den bandits (50, 30, 20, 80). Hoe wijkt de proefpersoon hier van af en hoe is dat te verklaren? (hint: kijk naar het keuzegedrag in de data file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_fitted_model(params):\n",
    "    # TO DO\n",
    "    return Q\n",
    "\n",
    "q_learn_fitted_model(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Two-armed Bandits\n",
    "\n",
    "\n",
    "We gaan nu kijken naar de resultaten van een ander experiment. Net als bij het vorige experiment is er telkens een keuze tussen opties die verschillende beloningen opleveren. In het eerdere voorbeeld waren het schatkisten, maar in de meeste experimenten zijn dit gokkasten (ook wel bandits genoemd omdat ze uiteindelijk met je geld er vandoor gaan). \n",
    "\n",
    "<img src=\"Images\\2bandits.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "In deze versie van het experiment heeft de proefpersoon telkens de keuze tussen twee machines. Anders dan de kisten, geven deze machines wel of geen beloning. De ene machine geeft de beloning met een grotere kans (80%) dan de andere (20%). \n",
    "Net als hierboven kan je in een data file (*RL_data2.csv*) terugvinden welke van de opties de proefpersoon gekozen heeft en hoeveel punten er verdiend zijn. Er waren in totaal 8 paren van bandits, dus 16 objecten in totaal. \n",
    "\n",
    "Lees de data in uit *RL_data2.csv* (staat in de Data map) met behulp van [loadtxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html):\n",
    "```python\n",
    "with open(\"Data\\RL_data2.csv\") as f:\n",
    "    data = np.loadtxt(f, dtype=int, delimiter=\",\", skiprows=1)\n",
    "```\n",
    "De headers in de file geven aan welke data er in de kolommen te vinden is: <br>\n",
    "`playerID` is de identificatie van de speler, `trial` is het trial nummer, `outcome` is de uitkomst van de keuze, `pairz` geeft aan welke paar zichtbaar was, waar paar 1 bestaat uit stimulus (object) 1 en 2, paar 2 uit stimulus 3 en 4, etc. Dan staat er in de kolom `choice` welke van de twee objecten gekozen is.\n",
    "\n",
    "Schrijf een nieuwe `q_learn_softmax_fit` functie zodat je deze op de data van de proefpersonen kan fitten. Ga er van uit dat de proefpersonen _geen_ ervaring heeft met dit type experiment en verwachten dat hij/zij gemiddeld  0 punten per ronde gaat verdienen (alle Q’s starten op 0). \n",
    "\n",
    "### Q6.a (5 punten)\n",
    "\n",
    "Fit het aangepaste model op de data van deze taak. Let er goed op dat voor het berekenen van de probabiliteit van het kiezen van een bepaalde stimulus je alleen rekening moet houden met de Q value van het alternatief binnen hetzelfde paar, niet de Q values van alle andere stimuli (zoals hierboven). Dus als pair 5 wordt aangeboden zie je alleen stim 9 en 10 en moet je alleen rekenen met $Q(9)$ en $Q(10)$.\n",
    "\n",
    "* Rapporteer de waardes van $\\alpha$, $\\theta$ en de Log Likelihood. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvements\n",
    "\n",
    "Deze simpele 2-armed Bandit taak wordt heel veel gebruikt om onderzoek te doen bij mensen en dieren. Deze taak is bijvoobeeld bij mensen en dieren afgenomen om leren in het brein te kunnen bestuderen. Het is een simpele taak, maar deze heeft veel inzichten gegeven in hoe mensen leren, inzichten die soms ook weer vertaald zijn naar de ontwikkeling van bijvoorbeeld DNNs en andere toepassingen in de kunstmatige intelligentie.\n",
    "\n",
    "Verbeteringen en uitbreidingen op het leermodel zijn vaak geïnspireerd door observaties van gedrag. Zoals we bij model fitting hebben geleerd, worden deze modellen getoetst door te kijken of de uitbreidingen ook echt leiden tot een betere fit. Hier ga je een aantal van deze verbeteringen bedenken en uitwerken. \n",
    "\n",
    "### Q6.b Dual Learning rates (5 punten)\n",
    "Het blijkt dat mensen en dieren anders reageren op beloning en straf (zie ook colleges week 1). Dat is iets wat niet in het standaard Q-learning model meegenomen is. Schrijf nu een versie van q-learning die dat wel doet: `q_learn_softmax_fit_dual`. Gezien de titel van deze vraag kan je al raden dat dit kan door twee verschillende learning rates ($\\alpha_{gain}$ en $\\alpha_{loss}$) te introduceren. Het makkelijkste is om voor deze opdracht deze twee learning rates te associeren met positive en negatieve prediction errors.\n",
    "\n",
    "* Rapporteer de waardes van $\\alpha_{gain}$, $\\alpha_{loss}$, $\\theta$ en de Log Likelihood. \n",
    "* Is dit model inderdaad een betere fit? Wat geeft het verschil in learning rates aan? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_softmax_fit_dual(params):\n",
    "    # TO DO\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.c Meta Learning (6 punten)\n",
    "Iets anders wat opvalt bij het leerpatroon van mensen is dat ze langzaam maar zeker de structuur van de taak zelf door krijgen. Dat wordt ook wel meta-leren genoemd. Ook dit is een belangrijk onderdeel van leren, en krijgt wederom veel aandacht binnen de AI gemeenschap (zie ook Lake et al. paper). Want door de structuur van een taak te leren kan er sneller en beter geleerd worden in toekomstige trials, maar deze kennis kan ook van toepassing zijn in nieuwe vergelijkbare situaties. \n",
    "\n",
    "Denk bijvoorbeeld aan het spelen van Super Mario. In het eerste level leer je veel over de structuur van het spel, paddestoelen zijn bijvoorbeeld goed en schildpadden slecht. Handige informatie voor in het volgende level, maar ook voor in een soortgelijk spel: Sonic the Hedgehog (in AI onderzoek zijn deze oude spelen weer helemaal populair). \n",
    "\n",
    "De two bandits taak is erg simpel, maar er zit wel een structuur in, namelijk: van elk paar is altijd één van de twee bandits goed en de ander slecht. Ook verschillen de kansen op positieve en negatieve feedback maar minimaal. Als je feedback krijgt over één van de twee opties in een paar, leer je eigenlijk ook al iets over de ander. \n",
    "\n",
    "Jouw opdracht is om het algoritme aan te passen op een manier dat het deze kennis van de taak gebruikt. Laat zien hoe het model fit op de data fit. Het gaat nu meer om de implementatie van de kennis over de taak, het is niet erg als het model minder goed fit. \n",
    "\n",
    "* Schrijf een nieuw algortime\n",
    "* Motiveer je keuzes\n",
    "* Evalueer je model (parameter waardes, en LogLikelihood). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.d Dynamic Learning rates (5 bonus punten)\n",
    "We zien vaak dat mensen in dit soort taken steeds minder reageren op de feedback, sommigen helemaal niet meer. Als je eenmaal weet welke van de twee objecten de beste is, dan moet je eigenlijk de negatieve uitkomsten die soms komen gewoon negeren. Klassieke Q-learning modelen die gebaseerd zijn op het Rescorla-Wagner model kunnen dat gedrag niet goed vangen. \n",
    "\n",
    "We hadden eerder al gezien dat het Pearce-Hall model een andere insteek had wat betreft de learning rate, namelijk dat deze afneemt naarmate de associatie sterkte toeneemt. Het gebeurt nu nog erg weinig maar dit kan ook binnen Q learning geimplementeerd worden.\n",
    "\n",
    "* Implemeneer de PH dynamic learning rate in het single learning rate Q-learning algoritme\n",
    "Voor het gemak:\n",
    "```python\n",
    "alpha = gamma*(abs(outcome - Q[choice]))+((1-gamma)*alpha)\n",
    "```\n",
    "\n",
    "Zorg dat je de scalar $S$ meeneemt in het model! Doe dit voor gemak in het single learning rate model. Wordt het er beter op? Wat is de waarde van gamma, en wat betekent dat? Vergelijk de fit van dit model dan dus ook met het single learning rate model zonder dynamic learnig rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learn_pearce_hall(params):\n",
    "    # TO DO\n",
    "    return -1*LL\n",
    "\n",
    "# TO DO: fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
