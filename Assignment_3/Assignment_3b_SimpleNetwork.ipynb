{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYr12ByeYoa9"
   },
   "source": [
    "# Assignment 3b Cognitive Modelling\n",
    "\n",
    "Dit is onderdeel van de derde opdracht voor Cognitive Modelling, assignment 3 bestaat in totaal uit 3 onderdelen:\n",
    "\n",
    "* Assignment 3a (43 punten)\n",
    "* __Assignment 3b (21 punten)__\n",
    "* Assignment 3c (23 punten)\n",
    "\n",
    "__Let op__: Opdracht 3b en 3c werken alleen in Google Colab!\n",
    "\n",
    "Geef antwoorden in blokken met code of met tekst.Gebruik voor antwoorden met tekst de \">\" voor blockquotes en geef bij elke vraag ook __kort uitleg__ als hier om wordt gevraagd. __Let op__: soms staan er meerdere vragen bij een onderdeel, lees de tekst dus nauwkeurig. \n",
    "\n",
    "Sla het uiteindelijke notebook op met jullie studentnummers en achternamen in de filenaam: `studentnummer_achternaam_opdrachtnummer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voorbereiding\n",
    "Voor deze opdracht zullen we een gratis Colab-account met Google opzetten. Hiervoor moet je een Google-account instellen als je er nog geen hebt. Vervolgens koppel je het Colab-account aan je Google Drive. In Colab kun je deep learning-toepassingen ontwikkelen met populaire bibliotheken zoals Keras, TensorFlow, PyTorch en OpenCV. Belangrijk is dat Colab gratis GPU-kracht biedt!\n",
    "\n",
    "### Stap 1: Maak een Google-account aan\n",
    "Het is een goed idee om je deep learning-experimenten te scheiden van je dagelijkse hoofdaccount, dus raden we je aan om een nieuw account aan te maken.\n",
    "\n",
    "Voor het gemak en om visualisaties van je resultaten offline uit te voeren, is het ook goed om Google backup-and-sync in te stellen. Als je het hebt ingesteld, kun je een map maken en de inhoud ervan op je lokale machine manipuleren. Eventuele wijzigingen worden automatisch gesynchroniseerd met je GoogleDrive, die ook toegankelijk is vanuit Colab. Dus je Colab-resultaten worden ook rechtstreeks gesynchroniseerd met je lokale machine.\n",
    "\n",
    "### Stap 2: Log in bij Colab\n",
    "Ga nu naar https://colab.research.google.com en koppel je Google Drive aan de Colab-omgeving. De eerste keer dat je dit doet, maakt het een map in je Drive met de naam Colab Notebooks. Plaats alle bestanden uit de map 'Bestanden voor Colab' (in de .zip die je hebt gedownload van Canvas) in die map Colab Notebooks op je Drive. Nu ben je klaar om te gaan (als dit niet werkt, open dan gewoon een nieuw notebookbestand, dit vraagt ook om de noodzakelijke stappen en bouwt de map).\n",
    "\n",
    "### Stap 3: Open het notebook Assignment_3b_SimpleNetwork.ipynb\n",
    "De gemakkelijkste manier om het notebook 'Assignment_3b_SimpleNetwork.ipynb' te openen, is door naar https://drive.google.com/ te gaan en met de rechtermuisknop op het bestand te klikken om het contextmenu te openen en het vervolgens als een Colab-project te openen.\n",
    "<img src=\"Images\\5.png\" style=\"max-width: 600px;\" alt=\"conect menu\"/>\n",
    "\n",
    "Dit zou het notebook in Colab moeten openen en we gaan vanaf daar verder. Nadat je klaar bent met 'Assignment_3b_SimpleNetwork.ipynb', kun je doorgaan naar 'Assignment_3c_Doom.ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss8-Qytypov-"
   },
   "source": [
    "<br>\n",
    "\n",
    "## 1. Q Networks\n",
    "\n",
    "We hebben nu gewerkt met een kleine wereld met maar maximaal 64 states. De echte wereld, en veel leuke spelletjes, hebben natuurlijk een veel grotere state space, en dan wordt het al snel erg lastig om nog een Q table te gaan bijhouden. Dit is waar Q netwerken heel handig zijn, en dan met name Deep Q Networks (DQN). Een deep neural network kan helpen de state space een stuk beter generaliseerbaar te maken, en vergelijkbare Q-values toekennen aan states die veel op elkaar lijken, wat een hele waardevolle eigenschap blijkt (zie bijv. de oorspronkelijke DQN paper over Atari games leren spelen [hier](https://arxiv.org/pdf/1312.5602v1.pdf)).\n",
    "\n",
    "\n",
    "__TensorFlow__\n",
    "\n",
    "In deze opdracht gaan we nu geen deep Q netwerk in elkaar zetten maar wel een simpel 1-layer Q netwerk, om te leren hoe deze werken en hoe het Q-learning algoritme kan worden geÃ¯mplementeerd in een netwerk. We maken hier gebruik van TensorFlow, een library waarmee je eenvoudig (deep) neural networks kan definiÃ«ren en trainen. Deze library is zeer geschikt voor DQN, en dus ook weer de FrozenLake omgeving.\n",
    "\n",
    "__FrozenLake__\n",
    "\n",
    "We gebruiken voor deze eerste uitleg weer de kleine frozenlake omgeving: `gym.make('FrozenLake-v0',map_name=\"4x4\",is_slippery=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVqH-9bLeiah",
    "outputId": "93a8ef8b-f7c5-478f-98b1-1cd39e4d7a67"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x # some version magic here. \n",
    "import tensorflow as tf      \n",
    "print(tf.__version__)\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# init Lake, somehow here we get the old version of gym, so FrozenLake-v0\n",
    "env_4x4 = gym.make('FrozenLake-v0',map_name=\"4x4\",is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCcJgcvFaMZ1"
   },
   "source": [
    "<br>\n",
    "\n",
    "We gaan een simpel Q netwerk opzetten waarbij er voor elke state (in totaal 16) een node is in de input laag, en voor elke actie (4) een node in the output laag. \n",
    "\n",
    "Het doel is om de gewichten (weights) van het netwerk zo te trainen dat een gegeven state als input correspondeert met de 4 output Q-values van de 4 verschillende acties voor die state.\n",
    "\n",
    "![](Images\\QNET.png)\n",
    "##### _in deze illustratie zie je het netwerk twee keer in verschillende states_\n",
    "\n",
    "\n",
    "Vervolgens kunnen we een *argmax* gebruiken om de actie met met hoogste Q-value te selecteren. Er zijn 16 input nodes, waarbij elke state nu gerepresenteerd kan worden met een ðŸ”¥[one-hot encoding](https://en.wikipedia.org/wiki/One-hot)ðŸ”¥, dus door alleen de input node van die state op 1 te zetten en alle andere input nodes op 0.\n",
    "\n",
    "De `Qout` laag is gedefineerd als de matrix vermenigvuldiging van inputs en weights, dus simpelweg de som van de inputs vermenigvuldig met de weights voor elke output (deze manier om een neural network te definiÃ«ren zou je bekend moeten voorkomen uit **Leren**). Er is hier dus *geen* activatiefunctie (zoals bijv. Sigmoid of ReLU), i.e. dit is een netwerk met alleen een lineaire layer.\n",
    "\n",
    "__Prediction error & learning__\n",
    "\n",
    "Heel erg vergelijkbaar met Q-learning zoals je het hiervoor gezien hebt is er een prediction error:\n",
    "\n",
    "$$\\delta = r_{t+1} + \\gamma\\ max_a\\ Q(s_{t+1} , a) âˆ’ Q(s_t , a_t)$$\n",
    "\n",
    "We gaan er dus weer even van uit dat in de volgene state $s_{t+1}$ de actie $a$ met de hoogste Q-value wordt gekozen (we hebben in de colleges gezien dat andere opties ook mogelijk zijn). \n",
    "\n",
    "Alleen nu is het niet zo dat de Q-values direct worden ge-update met de de prediction error $\\delta$ maar dat de gewichten van de verbindingen in het netwerk worden aangepast. Deze worden zo aangepast om de toekomstige prediction error te minimaliseren. \n",
    "\n",
    "Het gaat voor deze opdracht te ver om in te gaan op hoe de weights in het netwerk worden aangepast, maar als het goed is leer je dit bij een ander vak. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RW6x6Y9epsG"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "states = env_4x4.observation_space.n\n",
    "actions = env_4x4.action_space.n\n",
    "\n",
    "# These lines establish the feed-forward part of the network used to choose actions\n",
    "# A placeholder (tf.placeholder) is simply a variable that we will assign data to at a later date. \n",
    "# It allows us to create our operations and build our computation graph, without needing the data. \n",
    "# In TensorFlow terminology, we then feed data into the graph through these placeholders.\n",
    "inputs1 = tf.placeholder(shape=[1,states],dtype=tf.float32)\n",
    "# Setting the weights to random value\n",
    "W = tf.Variable(tf.random_uniform([states,actions],0,0.01))\n",
    "\n",
    "Qout = tf.matmul(inputs1,W) # we determine the Q values for each action by multiplying inputs by weights\n",
    "predict = tf.argmax(Qout,1) # \"predicted\" action is action with highest Qout (just like Q-learn above)\n",
    "\n",
    "# Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout)) # prediction error. nextQ ->targetQ   ###<- remove comment for student version\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.05) # let op hier is dus ook een learning rate ### remove comment for student version\n",
    "updateModel = trainer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3LGLRMhe7lD"
   },
   "outputs": [],
   "source": [
    "def q_network(env, num_episodes=250, num_rounds=99):\n",
    "    init = tf.initialize_all_variables()\n",
    "    # Set learning parameters\n",
    "    y = .97\n",
    "    e = .3\n",
    "\n",
    "    # Create lists to contain total rewards and steps per episode\n",
    "    jList = [] # list of steps taken in episode\n",
    "    rList = [] # list of rewards\n",
    "    aList = [] # list of actions\n",
    "    maxQnext = [] #list of Q values next state\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        W_init=(sess.run([W])) # save initial weights\n",
    "        for i in range(num_episodes):\n",
    "            #Reset environment and get first new observation\n",
    "            s = env.reset()\n",
    "            rAll = 0\n",
    "            d = False\n",
    "            j = 0\n",
    "            #The Q-Network, default run it for 99 rounds per episode\n",
    "            while j < num_rounds:\n",
    "                j+=1\n",
    "            \n",
    "                #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                a,targetQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(states)[s:s+1]})\n",
    "\n",
    "                # output a = maxQ of current state [s] and targetQ is list of all Q values in state [s]\n",
    "           \n",
    "                #e greedy:\n",
    "                if np.random.rand(1) < e:\n",
    "                    a[0] = env.action_space.sample()\n",
    "                aList.append(a[0])\n",
    "                \n",
    "                #Get new state and reward from environment\n",
    "                s1,r,d,_ = env.step(a[0])\n",
    "                \n",
    "                if d == True and r == 0: # falling in a hole will hurt \n",
    "                    r= -1\n",
    "  \n",
    "                #Obtain the Q values of the next state by feeding the new state through our network, \n",
    "                # and again assuming you will choose the action with the highest Q value\n",
    "                Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(states)[s1:s1+1]})\n",
    "            \n",
    "                #Obtain maxQ' of next state and use this to update the Q value of our chosen action.\n",
    "                maxQ1 = np.max(Q1)\n",
    "                maxQnext.append(maxQ1)\n",
    "                targetQ[0,a[0]] = r + y*maxQ1 #in list of targetQ values for s, update the Q value of chosen action\n",
    "\n",
    "                # targetQ, where you only updated the Q-value of the chosen action, is the set of Q-values that you want to network to \n",
    "                # output next time the agent is in state s. In a network we cannot directly update Qvalues because they are the output of the weights \n",
    "                # so we will tune the weights such that they will result in something closer to targetQ. \n",
    "                # The prediction error is thus the difference between targetQ and Qout\n",
    "                \n",
    "                \n",
    "                # Train our network using targetQ list, we try to adjust weights in order to \n",
    "                # minimize prediction error or squared error. \n",
    "                _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(states)[s:s+1],nextQ:targetQ})\n",
    "                rAll += r\n",
    "                s = s1\n",
    "                \n",
    "                if d == True:\n",
    "                    break\n",
    "            \n",
    "            #Reduce chance of random action as we train the model.\n",
    "            e = e*.995\n",
    "            jList.append(j)\n",
    "            rList.append(rAll)\n",
    "            \n",
    "        Weights=(sess.run([W])) # lets save the final weigts!\n",
    "   \n",
    "    print(\"\\nPercent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")\n",
    "    return (rList, jList, aList, W_init, Weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZOqZNQanWEl"
   },
   "source": [
    "<br>\n",
    "\n",
    "### __Q1.a__ (4 punten)\n",
    "Bestudeer bovenstaande code. Er zijn een aantal verschillen en overeenkomsten met standaard Q-learning met Q tables. Beantwoord de volgende vragen:\n",
    "\n",
    "1. Maakt dit algoritme een explore-exploit trade-off? Zo ja hoe?\n",
    "2. Is er een equivalent van de learning rate? \n",
    "3. Wat is de prediction error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7M75usXrjDy"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_ZByOPZ9qeZ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### __Q1.b__ (4 punten)\n",
    "We gaan het netwerk nu 1 keuze laten maken en goed kijken wat er precies gebeurt als we het onderstaande code blok runnen. Kijk goed naar de weights vÃ³Ã³r en na de eerste move. Wat verandert er? Hoe kan dit? En wat laat dit voor verschil zien met Q-nets vs Q-tables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gJ-nXUGe87Y",
    "outputId": "69209eca-0945-4a5d-bc5e-eaa835076d2b"
   },
   "outputs": [],
   "source": [
    "res=q_network(env_4x4, num_episodes=1, num_rounds=1)\n",
    "\n",
    "print(\"Inital weights:\")\n",
    "print(res[3][0][0])\n",
    "\n",
    "print(\"\\nreward:\", res[0][0])\n",
    "print(\"chosen action:\", res[2][0], \"\\n\")\n",
    "\n",
    "print(\"Weights after training\")\n",
    "print(res[4][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qTJuZF1sD8M"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIreuhhb_SnR"
   },
   "source": [
    "<br>\n",
    "\n",
    "We trainen nu nogmaals het netwerk maar nu met 500 episodes en 99 rounds per episode. We maken ook even twee grafiekjes om te zien hoe goed het model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "id": "KHyp19o0fER1",
    "outputId": "73785a6a-9978-40c9-da11-0944ef68bf19"
   },
   "outputs": [],
   "source": [
    "res=q_network(env_4x4, num_episodes=500)\n",
    "width = 20\n",
    "\n",
    "print('Rewards')\n",
    "res1 = np.asarray(res[0])\n",
    "result1 = res1[:(res1.size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "plt.plot(result1)\n",
    "plt.show()\n",
    "\n",
    "print('Steps')\n",
    "res2 = np.asarray(res[1])\n",
    "result2 = res2[:(res2.size // width) * width].reshape(-1, width).mean(axis=1)\n",
    "plt.plot(result2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1uo_wZK-3Al"
   },
   "source": [
    "<br> \n",
    "\n",
    "We zien dat het model heel snel leert wat de korste weg is. Natuurlijk gaat het nog niet altijd goed, ook omdat er een greedy beslis regel is zal er al af en toe een verkeerde keuze worden gemaakt. \n",
    "\n",
    "<br>\n",
    "\n",
    "### __Q1.c__ (3 punten)\n",
    "Laat nu ook zien wat de weights/Q-values zijn voor de vier mogelijke acties in het hokje links naast de frisbee (state 14). Heeft de beste actie ook de hoogste waarde?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "On73VbsmszPv",
    "outputId": "da0f08d3-0915-49f1-babd-7ac997b4e7a1"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_V6nzsNs0bB"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H308CAOJAN__"
   },
   "source": [
    "## 2. Working Memory --> Experience Replay\n",
    "\n",
    "Over de tijd zijn de Q_networks op vele manier uitgebreid om nog beter te kunnen leren. Een ding waar we straks naar gaan kijken is het toevoegen van meerdere lagen. Maar zo is er ook het idee van experience replay, iets wat gebaseerd is op hoe de hersens van mensen en dieren werken  (zie colleges).\n",
    "\n",
    "Experience is dat het netwerk, tussen leerepisodes door, zich herinnert wat hij hiervoor gedaan heeft en wat de uitkomst daar van was. Deze herinneringen worden dan weer gebruikt om van te leren (alsof elke herinnering een echte gebeurtenis was). Experience replay wordt veel gebruikt in deep reinforcement learning, en helpt daar het netwerk stabiele representaties vormen. \n",
    "\n",
    "Wat we hier gaan doen is experience replay toevoegen aan dit simpele Q_netwerk. Het zal dit netwerk niet meteen veel beter maken, omdat het probleem erg makkelijk is en het netwerk simpel, maar kan wel overzichtelijk het principe illustreren. \n",
    "\n",
    "Je hebt hier eigenlijk maar een aantal dingen voor nodig; een memory buffer \n",
    " \n",
    " `memory = []`\n",
    "\n",
    "In deze buffer sla voor elke ronde op wat er gebeurde. De staat waar in je was, welke actie je hebt ondernomen, of je een beloning kreeg, welke staat je terecht kwam en of dit het einde van de episode was):   \n",
    "\n",
    "`memory.append((s, a[0], r, s1, d))`\n",
    "\n",
    "Dan, aan het eind van elke episode, haal je 30 willekeurige herinneringen boven en speelt deze weer uit alsof het echt gebeurde (dus zorgt voor een zelfde update in weights als normaal leren). Bij het begin van een nieuwe leer episode wordt de buffer weer leeggemaakt om ruimte te maken voor nieuwe evaringen. \n",
    "``` python\n",
    "for k in range(30):              \n",
    "    [(s, a, r, s1, d)] = random.sample(memory, 1) #get random memory\n",
    "                    \n",
    "    # calcuate prediction error and train network\n",
    "    targetQ = sess.run(Qout,feed_dict={inputs1:np.identity(states)[s:s+1]})\n",
    "    Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(states)[s1:s1+1]})\n",
    "    maxQ1 = np.max(Q1)\n",
    "    targetQ[0,a ]= r + y*maxQ1\n",
    "    _, W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(states)[s:s+1],nextQ:targetQ})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOqjCWyEuH78"
   },
   "source": [
    "### __Q2.a__ (4 punten)\n",
    "Implementeer deze experience replay, door de bovenstaande code op de juiste plekken in `q_network()` te plakken.  Vergelijk de prestatie van het model met en zonder replay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTmfK8sYtl6Q"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5F_ioIAtpch"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnfqsK9tEtel"
   },
   "source": [
    "### __Q2.b__ (4 punten)\n",
    "\n",
    "We hebben nu naar replay gekeken waarbij je naar willekeurige herinneringen kijkt van de laatste 30 rondes. Het zal waarschijnlijk in deze context niet veel hebben opgeleverd. Ook is dit misschien niet de beste manier van herinneringen ophalen. \n",
    "\n",
    "\n",
    "Er zijn verschillende algoritmes bedacht waarbij niet naar willekeurige, maar juist naar specifieke herinneringen werd gekeken om het leren nog verder te optimaliseren. Bedenk een optimalisatie van de replay functie, en beschrijf hoe je dat zou implementeren. Schrijf in je antwoord je motivatie voor je aanpassing (die is belangrijker dan het slagen er van, natuurlijk kan je als je wilt laten zien dat je netwerk beter of slechter werkt maar dat hoeft niet). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKqp7MDSt9JH"
   },
   "source": [
    "> *Antwoord*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3b_SimpleNetwork_Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
