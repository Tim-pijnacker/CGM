{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRuKz2O2yhwJ"
   },
   "source": [
    "# Assignment 3acCognitive Modelling\n",
    "\n",
    "Dit is onderdeel van de derde opdracht voor Cognitive Modelling, assignment 3 bestaat in totaal uit 3 onderdelen:\n",
    "\n",
    "* Assignment 3a (43 punten)\n",
    "* Assignment 3b (21 punten)\n",
    "* __Assignment 3c (23 punten)__\n",
    "\n",
    "__Let op__: Opdracht 3b en 3c werken alleen in Google Colab!\n",
    "\n",
    "Geef antwoorden in blokken met code of met tekst.Gebruik voor antwoorden met tekst de \">\" voor blockquotes en geef bij elke vraag ook __kort uitleg__ als hier om wordt gevraagd. __Let op__: soms staan er meerdere vragen bij een onderdeel, lees de tekst dus nauwkeurig. \n",
    "\n",
    "Sla het uiteindelijke notebook op met jullie studentnummers en achternamen in de filenaam: `studentnummer_achternaam_opdrachtnummer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-pWfxZXh10s"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Voorbereiding stap 1. De Colab environment\n",
    "\n",
    "Koppel je notebook aan een GPU: ga naar **Edit > Notebook settings** of **Runtime > Change runtime type** en selecteer GPU as Hardware accelerator. \n",
    "\n",
    "![screenshot](https://drive.google.com/uc?id=1QsJXUPPL0eEoWHI7a-A1_F42djdgCcBL)\n",
    "\n",
    "and daarna:\n",
    "\n",
    "![screenshot](https://drive.google.com/uc?id=1ODvXSxF7OBy9blmZfC1Yk870uw80c6YW)\n",
    "\n",
    "Nu moet je jouw Google Drive koppelen aan deze omgeving zodat je daarvan kan lezen en naar kan schrijven. Het zal misschien nodig zijn om een key in te voeren. Klik daarvoor op de link die verschijnt. Die brengt je naar een google pagina, en copy paste dan je key hier in het notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAM90GMkrhSf",
    "outputId": "b2961bfe-cf22-475e-b504-9d7d665309bb"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yN223blcJqsW",
    "outputId": "932e3b88-214a-4df4-adb5-58d31eb20de4"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/'Files for Colab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-jKx-wvGMwp"
   },
   "source": [
    "Check even of alle files er zijn die je nodig hebt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jvSZIxxuwBSR",
    "outputId": "39b64aeb-dd3e-49f8-ea8b-2bb1ef0660c7"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoX3yT3MofKl"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Voorbereiding stap 2. Installeer Vizdoom ‚òï\n",
    "\n",
    "Nu moeten we Vizdoom en een paar dependencies installeren op de Colab. Deze twee stappen kunnen enkele minuten duren (tijd voor koffie). Let op dat als je de colab instance een tijdje niet gebruikt, je deze stap opnieuw moet doen! Net als de key for google drive geven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5HzGGb6Ht2lC",
    "outputId": "40e14d47-9ff9-4750-92d2-0cac8d056bbd"
   },
   "outputs": [],
   "source": [
    "## to get vizdoom to work \n",
    "## https://stackoverflow.com/questions/50667565/how-to-install-vizdoom-using-google-colab \n",
    "## takes a few minutes.... \n",
    "%%bash\n",
    "# Install deps from https://github.com/mwydmuch/ViZDoom/blob/master/doc/Building.md#-linux\n",
    "apt-get update\n",
    "\n",
    "apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev \\\n",
    "nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev \\\n",
    "libopenal-dev timidity libwildmidi-dev unzip\n",
    "\n",
    "# Boost libraries\n",
    "apt-get install libboost-all-dev\n",
    "# Lua binding dependencies\n",
    "apt-get install liblua5.1-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k7MHMAhIec9"
   },
   "source": [
    "En dan nu Vizdoom (ongeveer 5 minuten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zc3lMLFPu9rK",
    "outputId": "4fa5530c-db5f-45c4-bb51-3f048b24d1a9"
   },
   "outputs": [],
   "source": [
    "# this takes a few minutes more.. \n",
    "!pip install vizdoom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdHWDZwUoFHO"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Voorbereiding stap 3. Setting up the Game environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_zYZfXnTrKUv",
    "outputId": "aec99f1c-dd0a-4b8f-bc02-cfeb22203f76"
   },
   "outputs": [],
   "source": [
    "# TF 1 code, just got deprecated! We go to TF1 for this exercise\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf        # Deep Learning library\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np              # Handle matrices\n",
    "from vizdoom import *           # Doom Environment\n",
    "import random                   # Handling random number generation\n",
    "import time                     # Handling time calculation\n",
    "from skimage import transform   # Help us to preprocess the frames\n",
    "from collections import deque   # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings                 # This ignore all the warning messages that are normally printed during the training\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 851
    },
    "id": "2RrNX8urkO8-",
    "outputId": "3b401e1b-5cdc-4ab0-e844-a7a54c9a5162"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('Images/doom.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX3Y6l9u7O6K"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## De regels van het spel üéÆ\n",
    "Nu we alle libraries hebben, kunnen we de regels van het spel bepalen. Doom wordt bepaald door:\n",
    "  - Een  `configuration file` \"basic.cfg\" daar in vinden alle opties (frame size, mogelijke handelingen)\n",
    "  - Een `scenario file`: \"basic.wad\" deze genereerd het correcte scenario (ruimte, monsters etc, komt bij vizdoom) .\n",
    "\n",
    "Open beide files met een text editor en bestudeer ze kort, kijk of je de belangrijkste elementen kan ontcijferen (en later mogelijk veranderen). Het spel ziet er als volgt uit:\n",
    "\n",
    "__De omgeving:__\n",
    "- Een monster staat op een willekeurige plek aan de overkant van het veld \n",
    "- De speler heeft drie mogelijke acties: naar links, naar rechts en schieten \n",
    "- 1 hit is genoeg om het monster uit te schakelen\n",
    "- Een episode is over als het monster is geraakt of door een timeout (300).\n",
    "\n",
    "__De rewards:__\n",
    "- -5 punten voor het schieten van kogel  \n",
    "- -1 punt voor elke tijdstap (energy consumption)\n",
    "- +100 punten voor het uitschakelen van het monster uitschakelen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMHZz40HYS-r"
   },
   "outputs": [],
   "source": [
    "# Hier definieren we de omgeving\n",
    "\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    ## we cant watch the game perform on collab\n",
    "    game.set_window_visible(False)\n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ouzfcQkxwg35"
   },
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjtC1sWKTbHz"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Preprocessing ‚öôÔ∏è\n",
    "\n",
    "Preprocessing is een belangrijke stap, omdat we de complexiteit van onze data willen verminderen om zo de rekentijd die nodig is voor training te verminderen. In termen van leren kan je deze stap zien als een leraar die de leerling alleen vertelt in welke richting te kijken. Minimale supervision, maar dit helpt de leerling wel door de aandacht te vestigen in de juiste richting.\n",
    "\n",
    "__Preprocessing stappen:__\n",
    "- Grijstinten maken van alle frames (omdat kleur geen belangrijke informatie toevoegt). Dit wordt al gedaan door het configuratiebestand (!).\n",
    "- Het scherm bijsnijden (in ons geval verwijderen we het dak omdat het geen informatie bevat)\n",
    "- Het normaliseren van de pixelwaarden \n",
    "- Het verkleinen van het preprocessed frame.\n",
    "\n",
    "Dit resulteert in een 84x84 grayscale frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "WqARVmXB27dd",
    "outputId": "fab6340b-c050-497b-a68c-5c54879838c8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame, resize it from\n",
    "         _________________\n",
    "        |                 |        __________  \n",
    "        |                 |       |          |\n",
    "        |                 |       |          |\n",
    "        |                 |       |          |\n",
    "        |_________________|  to   |__________|\n",
    "        \n",
    "    Normalize it,     \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame\n",
    "\n",
    "Image('Images/preproc.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRanygxZTfC6"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Stacked Frames üïñüïóüïòüïô\n",
    "\n",
    "Als input states gaan we dus nu geen single frames gebruiken, maar een stapeltje (stacked) frames. Het stapelen van frames is erg belangrijk omdat het een gevoel van beweging/tijd meegeeft aan het neurale netwerk. Eerst preprocessen we het frame, vervolgens voegen we het frame toe aan een *deque* dat automatisch het oudste frame verwijdert, dan bouwen we de state (S) van deze frames.\n",
    "\n",
    "Dit is hoe de functie werkt:\n",
    "- Voor de start state voeren we 4 keer hetzelfde begin frame in\n",
    "- Bij elke tijdsstap voegen we het nieuwe frame toe aan de deque, en wordt het oudste frame weggehaald. Deze nieuwe stack is dan de nieuwe state\n",
    "- Enzovoort\n",
    "- Als een episode voorbij is maken we een nieuwe stapel met 4 nieuwe frames (omdat we ons in een nieuwe episode bevinden).\n",
    "\n",
    "Hier is nog een meer in depth artikel over stacked frames:  <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  Frame skipping and preprocessing</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drEJdp2hxP1a"
   },
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYGxzlmDT6ho"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Initialiseren van hyperparameters üéõ\n",
    "In dit deel stellen we de verschillende hyperparameters in. Eerst definieren we de hyperparameters van het neurale netwerk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1XP6Xt2RoyH"
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]     \n",
    "action_size = game.get_available_buttons_size()    # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.002      # Alpha (aka learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7FbiksuSl93"
   },
   "source": [
    "### ‚ùì __Q1 (2 punten)__\n",
    "Waarom is de input voor het model, `State_size`,  een matrix van 84x84x4 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCOfwPKH1PPb"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTciyfOgdbk2"
   },
   "source": [
    "---\n",
    "\n",
    "## Deep Q-learning Neural Network model bouwen üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQ9o01qi8IzZ"
   },
   "source": [
    "Dit is de structuur van het Deep Q-learning model (zie ook de afbeelding hieronder):\n",
    "- We nemen de stacked frames as input \n",
    "- Deze input gaat door 3 convnets\n",
    "- De data wordt platgeslagen (flattened), en gaat dan door twee fully connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "UAGO9ABibFG-",
    "outputId": "f8bb9278-366d-42b8-c3c9-d61423751372"
   },
   "outputs": [],
   "source": [
    "Image('Images/model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dd-vsD_0VWcB"
   },
   "source": [
    "Bestudeer de code onder vraag 3 bij `class DQNetwork`, kijk in hoeverre je de structuur zoals hier boven beschreven terug kan lezen. Het is nu niet belangrijk dat alle details helemaal begrijpt, maar wel de grote lijnen. \n",
    "\n",
    "<br>\n",
    "\n",
    "### ‚ùì__Q2. Network Questions (4 punten)__\n",
    "\n",
    "1. Hoeveel nodes heeft de laatse laag, en wat representeert deze laag?\n",
    "2. Welke activatie functie gebruikt het netwerk?\n",
    "3. En welke loss functie wordt er geminimalizeerd?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suAyFQpJ1vyA"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VF8QjVeEYstI"
   },
   "source": [
    "<br>\n",
    "\n",
    "Als het goed is heb je bij Computer Vision alles over Convnets geleerd en ook bij andere vakken over ANNs. Maar om toch even kort terug naar de theorie. Waarom gebruiken we hier niet een simpel one-layer network, of waarom geen Q-tables zoals we hiervoor hebben gedaan? De reden is de grootte van de state space.\n",
    "\n",
    "\n",
    "### ‚ùì__Q3. (2 punten)__ \n",
    "Hoe groot is de state space (na pre-processing) en hoeveel state, action pairs zijn er dus? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4WuUQyH2C1Z"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6tTBexOhMt4"
   },
   "source": [
    "<br>\n",
    "\n",
    "Precies! Dat is al heel erg groot en dit spel is nog steeds erg beperkt. Stel je voor dat je nog door de ruimte kan bewegen etc. We zagen al dat het grotere Frozen Lake van 8x8 lastig was voor Q-tables. Dit is niet te doen. Convnets kunnen de agent de state space laten doorgronden en laten generalizeren.\n",
    "\n",
    "Een convnet kan gebruik maken van spatiele relaties: denk bijvoorbeeld aan dat als het monster twee stappen links van jou staat, je altijd LEFT, LEFT, SHOOT zou moeten doen, ongeacht of het monster helemaal links staat of in het midden. Als je deze relaties kan gebruiken om te leren is dat dus heel handig. Een Q-table aanpak zou voor state monster_links en state monster_midden geheel opnieuw moeten leren wat de beste actie is. \n",
    "\n",
    "Meer lezen over convolutional nets: \n",
    "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2xHQEcMxUJj"
   },
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + y*max Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:       CNN\n",
    "            BatchNormalization:  ELU\n",
    "            \"\"\"\n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")  # --> [20, 20, 32]\n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:      CNN\n",
    "            BatchNormalization:  ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\") # --> [9, 9, 64]\n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:       CNN\n",
    "            BatchNormalization:  ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\") # --> [3, 3, 128]\n",
    "            self.flatten = tf.layers.flatten(self.conv3_out) # --> [1152]\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                      units = 512,\n",
    "                                      activation = tf.nn.elu, \n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc1\")\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3,\n",
    "                                          activation=None)\n",
    "\n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUUx9ZTDxXRp",
    "outputId": "49836e9c-fbfd-4542-c7bf-abc0b1c6073d"
   },
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)\n",
    "# ignore warnings about TF 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEG2d7a1Rs8U"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### De hyperparameters voor de training üéõ\n",
    "In dit deel voegen we de trainingshyperparameters toe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRCHZ0cgxRSF"
   },
   "outputs": [],
   "source": [
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 501          # Total episodes for training\n",
    "max_steps = 100               # Max possible steps in an episode (let op max is 300 in game)\n",
    "batch_size = 64               # size of batch used for replay\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0           # exploration probability at start\n",
    "explore_stop = 0.01           # minimum exploration probability \n",
    "decay_rate = 0.0001           # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95                  # Discounting rate (zie lectures Temporal Difference learning)\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size  # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000         # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bh8VzP4VzRF"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Experience Replay üîÅ\n",
    "Nu we ons neurale netwerk hebben gemaakt, gaan we de Experience Replay-methode implementeren. Eerst maken we een deque Memory-object. Een deque (dubbele wachtrij) is een gegevenstype dat het oudste element verwijdert telkens wanneer er een nieuw element wordt toegevoegd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Yyk48ehxf90"
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tQDApOoWG-b"
   },
   "source": [
    "Hier zullen we het probleem aanpakken dat de agents beginnen met een volledig leeg geheugen: we vullen ons geheugen vooraf in door willekeurige acties te ondernemen en de ervaring op te slaan (*state, action, reward, new_state*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PLTbHnUxjus"
   },
   "outputs": [],
   "source": [
    "memory = Memory(max_size = memory_size)    # Instantiate memory\n",
    "game.new_episode()                         # Render the environment\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    \n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        state = game.get_state().screen_buffer                             # First we need a state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)  # Stack the frame\n",
    "    \n",
    "    action = random.choice(possible_actions)  # Random action\n",
    "    reward = game.make_action(action)         # Get the rewards\n",
    "    done = game.is_episode_finished()         # Check if the episode is finished\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)                                 # We finished the episode\n",
    "        memory.add((state, action, reward, next_state, done))              # Add experience to memory\n",
    "        game.new_episode()                                                 # Start a new episode        \n",
    "        state = game.get_state().screen_buffer                             # First we need a state\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)  # Stack the frame\n",
    "        \n",
    "    else: \n",
    "        next_state = game.get_state().screen_buffer                                  # Get the next state\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False) # Stack the frame\n",
    "        memory.add((state, action, reward, next_state, done))                        # Add experience to memory\n",
    "        state = next_state                                                           # Our state is now the next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsGauEasWMmB"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Tensorboard üìä\n",
    "\n",
    "We gebruiken deze functie om de data van de training weg te schrijven. Voor meer info zie deze <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">tutorial</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFo3UjqW3ETU"
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")    # Setup TensorBoard Writer\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)               # Losses\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NU9MOg3WY4a"
   },
   "source": [
    "Hieronder defini√´ren we de keuzefunctie van de agent. We implementeren een  klassieke e-greedy keuzeregel met exponenti√´le afname in exploratie. Een nieuwigheid is dat er een ondergrens is voor de parameter **explore_stop**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRbsJDRr3HSR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    exp_exp_tradeoff = np.random.rand()                 # First we randomize a number\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        action = random.choice(possible_actions)        # Explore\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation), estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnBFg18sW0si"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Training üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Ons algoritme:\n",
    "<br>\n",
    "* Initialiseer de weigths van het netwerk\n",
    "* Initialiseer de game omgeving\n",
    "* Initialiseer de decay rate voor e-greedy\n",
    "<br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0 \n",
    "    * Observe the first state $s_0$ \n",
    "    <br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate (decreas decay)\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "\n",
    "Je ziet dat dit eigenlijk precies hetzelfde is als we eerder in het FrozenLake Q-net hadden geimplementeerd, alleen zitten er nu dus wel wat lagen tussen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qqkyJ4_2rbg"
   },
   "source": [
    "<br>\n",
    "\n",
    "### ‚ùì __Q4.a (7 punten)__\n",
    "\n",
    "Je gaat hieronder de agent trainen in 500 episodes met de huidige set hyperparameters, dit kan wel ongeveer 10 minuten duren, dus tijd om even pauze te nemen en de benen te strekken. Bekijk en rapporteer na afloop het gedrag van de agent tijdens het leerproces. Dit kan op twee manieren:\n",
    "\n",
    "1. Aan de hand van de score die de agent behaalt tijdens het leren;\n",
    "2. Aan de hand van hoe de agent het spel speelt (visueel).\n",
    "\n",
    "__1. Score__\n",
    "  \n",
    "  * Tijdens het trainen schrijft het script de output weg in **test.txt** die kan worden gebruikt voor visualisatie van de leercurve(s). Deze wordt dus in je Google Drive geschreven. Geef deze file een andere naam die specifiek is voor setttings van het netwerk dat je traint, zo kan je later modelen vergelijken.\n",
    "  * Maak twee plots op basis van deze data: de leercurve met de score van de agent tijdens het leren, de kans op een exploratie. Zet in de grafieken de gemiddelde punten en explore probabilities voor bins van 10 episodes om de leercurven te illustreren en rapporteer ook het totaal aantal punten. Beschrijf wat je ziet.\n",
    "\n",
    "__2. Visualisatie__ \n",
    "\n",
    "* De code hieronder slaat het model om de 50 episodes `model_XX.ckpt.` op in de directory `models`. Dit is de staat van het model na XX episodes trainen. De filenames specificeren dus weer het model. \n",
    "\n",
    "  Met de code onder vraag 5 kan je de boven genoemde outputfiles gebruiken om naar gedrag van de agent te kijken, gegeven de weights in het netwerk na een een XX aantal leer episodes. Deze code laat slechts √©√©n game zien (√©√©n episode), dus doe dit misschien een aantal keer omdat het anders net zo kan uitvallen dat de game begint dat de agent recht voor de alien staat en meteen neerschiet. Dat is goed maar vertelt weinig over het gedrag. \n",
    "\n",
    "  Naast deze ene game, slaat deze code ook de sequentie van states als lijst van screenshots op in `trajectory`. Deze `trajectory` kan je in een animatie te maken door ze achter elkaar te plakken. Je kan deze  bekijken, en ook opslaan als mp4.\n",
    "  \n",
    "* Beschrijf het verschil in gedrag in drie verschillende fasen (episode 50, 150 en 500). Wat voor soort fouten zie je? Sla de vier .mp4 bestanden van deze drie fases op en __lever de bestanden in__.\n",
    "\n",
    "_Let op: voor sommigen werkt de functie_ `save_path = saver.save(sess, \"./models/model_{}.ckpt\".format(episode)` _niet altijd goed. Het is niet duidelijk waarom, maar dan kan je dit deel van de opdracht helaas niet helemaal doen, laat dit weten aan je TA._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lebPEshZl724"
   },
   "outputs": [],
   "source": [
    "f= open(\"test.txt\",\"a+\") # <- verander deze filename!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyjtRtpg3Ma8",
    "outputId": "807ad6ee-03dd-46ad-f0b5-a3fb6c47db43"
   },
   "outputs": [],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())     # Initialize the variables \n",
    "        decay_step = 0                                  # Initialize the decay rate (that will use to reduce epsilon)\n",
    "        game.init()                                     # Init the game\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            step = 0                                    # Set step to 0\n",
    "            episode_rewards = []                        # Initialize the rewards of the episode\n",
    "            game.new_episode()                          # Make a new episode and observe the first state\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                decay_step +=1 # Increase decay_step\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                reward = game.make_action(action)        # Do the action\n",
    "                done = game.is_episode_finished()        # Look if the episode is finished\n",
    "                episode_rewards.append(reward)           # Add the reward to total reward\n",
    "                total_reward = np.sum(episode_rewards)   # Get the total reward of the episode\n",
    "                \n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    next_state = np.zeros((84,84), dtype=np.int) # the episode ends so no next state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    step = max_steps  # Set step = max_steps to end the episode                    \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer             # Get the next state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    memory.add((state, action, reward, next_state, done))   # Add experience to memory\n",
    "                    state = next_state                                      # st+1 is now our current state\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                batch = memory.sample(batch_size)   # Obtain random mini-batch from memory\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                 # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                        DQNetwork.target_Q: targets_mb,\n",
    "                                                        DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "            \n",
    "            print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "            \n",
    "            f.write('Episode: {}'.format(episode) + \"\\t\" +\n",
    "                              'Total reward: {}'.format(total_reward) + \"\\t\"+\n",
    "                              'Explore P: {:.4f}'.format(explore_probability) + \"\\t\"+\n",
    "                               'Training loss: {:.4f}'.format(loss) + \"\\n\")\n",
    "                              \n",
    "            # Save model every 50 episodes\n",
    "            if episode % 50 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model_{}.ckpt\".format(episode))\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vTFFX0AtC3W"
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHczGZgSGrHX"
   },
   "outputs": [],
   "source": [
    "# TO DO plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv1WqwQu8Izd"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I25DJSq93d9I"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùìQ4.b (6 punten)\n",
    "Probeer nu de prestaties van het model te verbeteren en voer bovenstaande stappen nog eens uit. Dit kan je ondertussen op heel veel manieren doen, je kan bijvoorbeeld veranderingen maken aan de hyperparameters, exploratie regel, replay etc. \n",
    "\n",
    "* Leg uit waarom je verwachtte dat jouw aanpassing het beter zou maken\n",
    "* Laat zien of het werkte op basis van traingsgrafiek (stap 1 hierboven)\n",
    "* Kijk goed naar het gedrag na 500 episodes leren of je daar iets van kan afleiden (stap 2 hierboven). \n",
    "\n",
    "Het aantal trainings episodes is technische gezien een hyperparameter, maar die bedoelen we dus niet :)\n",
    "\n",
    "**Let op:** \n",
    "1. Elke keer voordat je een nieuwe training uitvoert met nieuwe hyperparameters moet je de game opnieuw initialseren: <br>\n",
    "``\n",
    "game, possible_actions = create_environment()\n",
    "`` <br>\n",
    "\n",
    "2. Je moet ook de weights van het netwerk resetten (model resetten door opnieuw initialisatiestappen uit te voeren), begin bij <br> ``tf.reset_default_graph()\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)`` (net boven het kopje *'De hyperparameters voor de training'*).<br>\n",
    "3. Reset ook het geheugen (bij het kopje *'Experience replay'*)\n",
    "\n",
    "Dan is het geheugen van de CoLab instantie (RAM) ook weer leeg en het netwerk klaar om te gaan leren. Ook nu gaat het om je idee, niet meteen of het netwerk ook beter leert, maar dat is natuurlijk wel een mooie uitkomst!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5iGAmYmK8Izd"
   },
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xpwx3MY35Vz"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì__Q5 (2 Punten)__\n",
    "Bestudeer onderstaande code, hoe verschilt de decision policy van de gedrags simulatie met die van de training agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d55ggqRJ8Ize"
   },
   "source": [
    "> *Antwoord*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DiUfC4u8Izd",
    "outputId": "f55e5b49-2344-448d-a707-2a3e4e21e83d"
   },
   "outputs": [],
   "source": [
    "## Dit codeblock en het volgende heb je ook nodig om het tweede deel van Q4.a te beantwoorden\n",
    "trajectory = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_environment()\n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model_350.ckpt\") #<- change this to check behavior after different learning episodes\n",
    "    game.init()\n",
    "    for i in range(1):\n",
    "        \n",
    "        done = False\n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            # Get Q values for each action\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Select action\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "                #print(\"step\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                trajectory.append(next_state)\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "ZmDdgU0oLJNF",
    "outputId": "807faf39-d0fc-4187-80eb-15ae53b6c400"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "rc('animation', html='html5')\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def animate(i):\n",
    "    ax.imshow(trajectory[i])\n",
    "    return ax\n",
    "\n",
    "gif = animation.FuncAnimation(fig, animate, frames=len(trajectory))\n",
    "gif\n",
    "gif.save('DOOM_350.mp4', fps=5, dpi=200) # comment out gif.save and rerun block to see amin in line, or just check your mp4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLgfIs5vR8fV"
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Smart Exploration & Beyond..\n",
    "Je hebt hier gezien dat we weer e-greedy gebruikte om met de explore exploit trade-off om te gaan. Maar we hadden eerder ook al wel gezien dat dit niet altijd de slimste of beste manier van exploratie is. We hebben UCB besproken die niet bezochte states een exploratie bonus geeft, en die bonus neemt af naarmate er meer over een staat geleerd is. Zo kom je niet meer terug bij \"slechte\" states.\n",
    "\n",
    "Nu willen we niet alle mogelijk states per se bezoeken, van sommige kunnen we misschien al direct voorspellen dat deze niets gaan opleveren. In deze context is Curiosity Based Learning dus heel interessant. Kijk nogmaals naar [Curiosity driven learning](https://towardsdatascience.com/curiosity-driven-learning-made-easy-part-i-d3e5a2263359) voor een goede uitleg. \n",
    "\n",
    "Het gaat nu te ver om hier dieper op in te gaan maar ik hoop dat door de basis te snappen van DQNs en dat dit je nu al een beter idee hebt hoe deze algoritmes werken. Probeer ook zeker zelf verschillende games van Vizdoom uit, of andere games zoals Space invaders. Er zijn eindeloos veel tutorials te vinden op het internet."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3c_Doom.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
